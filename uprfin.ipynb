{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrwiwWKSGDPr4JRlxyDoXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielaaltnv/pli/blob/main/uprfin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbL86SA-oWvB"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange, tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import ElectraTokenizer, ElectraForTokenClassification\n",
        "from torch import nn, optim\n",
        "from google.colab import files\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "SEQ_LEN = 128\n",
        "VALIDATION_SPLIT = 0.05\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "try:\n",
        "    with open(\"train_data.utf8\", \"r\", encoding=\"utf-8\") as f:\n",
        "        pass\n",
        "except FileNotFoundError:\n",
        "    uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "\n",
        "text_data = text_data.replace(\"\\n\", \" \")\n",
        "text_data_clean = re.sub(r'[^a-zA-Z0-9.,!?áčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ ]', '', text_data)\n",
        "text_data_clean = re.sub(r'\\s+', ' ', text_data_clean).strip()\n",
        "words = text_data_clean.split(\" \")\n",
        "split_index = round(len(words) * (1 - VALIDATION_SPLIT))\n",
        "train_data = words[:split_index]\n",
        "\n",
        "\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\"Seznam/small-e-czech\")\n",
        "\n",
        "\n",
        "def tokenize_with_labels(data, tokenizer):\n",
        "    input_ids = []\n",
        "    label_ids = []\n",
        "\n",
        "    for word in tqdm(data, desc=\"Tokenizing\"):\n",
        "        if word == \"\":\n",
        "            continue\n",
        "        label = 1 if word[0].isupper() else 0\n",
        "        tokens = tokenizer.tokenize(word)\n",
        "        if not tokens:\n",
        "            continue\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_ids.extend(token_ids)\n",
        "        label_ids.append(label)\n",
        "        label_ids.extend([-100] * (len(token_ids) - 1))  # Mask out sub-tokens\n",
        "\n",
        "    return input_ids, label_ids\n",
        "\n",
        "input_ids, label_ids = tokenize_with_labels(train_data, tokenizer)\n",
        "\n",
        "\n",
        "def batch_generator(inputs, labels, batch_size, seq_len):\n",
        "    while True:\n",
        "        for i in range(0, len(inputs) - seq_len * batch_size, batch_size * seq_len):\n",
        "            batch_inputs, batch_masks, batch_labels = [], [], []\n",
        "            for j in range(batch_size):\n",
        "                start = i + j * seq_len\n",
        "                end = start + seq_len\n",
        "                batch_inputs.append(inputs[start:end])\n",
        "                batch_labels.append(labels[start:end])\n",
        "                batch_masks.append([1] * seq_len)\n",
        "            yield (\n",
        "                torch.tensor(batch_inputs).to(device),\n",
        "                torch.tensor(batch_masks).to(device),\n",
        "                torch.tensor(batch_labels).to(device),\n",
        "            )\n",
        "\n",
        "\n",
        "model = ElectraForTokenClassification.from_pretrained(\"Seznam/small-e-czech\", num_labels=2).to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor([1.0, 5.0]).to(device))\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "loss_vals, f1_vals = [], []\n",
        "running_loss = 0\n",
        "running_f1 = 0\n",
        "\n",
        "\n",
        "train_gen = batch_generator(input_ids, label_ids, BATCH_SIZE, SEQ_LEN)\n",
        "model.train()\n",
        "t = trange(50000, desc=\"Training\")\n",
        "\n",
        "for i in t:\n",
        "    inputs, masks, labels = next(train_gen)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids=inputs, attention_mask=masks)\n",
        "    logits = outputs.logits.permute(0, 2, 1)\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    preds = torch.argmax(logits.detach().cpu(), dim=1).view(-1)\n",
        "    targets = labels.detach().cpu().view(-1)\n",
        "\n",
        "\n",
        "    f1 = f1_score(preds, targets, average=None, labels=[1], zero_division=0)\n",
        "    running_f1 += f1[0]\n",
        "\n",
        "    t.set_description(f\"Loss: {running_loss/(i+1):.4f} | F1: {running_f1/(i+1):.4f}\")\n",
        "\n",
        "\n",
        "    if (i + 1) % 500 == 0:\n",
        "        loss_vals.append(running_loss / (i + 1))\n",
        "        f1_vals.append(running_f1 / (i + 1))\n",
        "\n",
        "        ax1.clear()\n",
        "        ax2.clear()\n",
        "        ax1.set_title('F1 score every 500 batches')\n",
        "        ax1.set_xlabel('Batch (x500)')\n",
        "        ax1.set_ylabel('F1')\n",
        "        ax2.set_title('Loss every 500 batches')\n",
        "        ax2.set_xlabel('Batch (x500)')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax1.plot(f1_vals, '-o', label='F1')\n",
        "        ax2.plot(loss_vals, '-o', label='Loss')\n",
        "        ax1.legend()\n",
        "        ax2.legend()\n",
        "        clear_output(wait=True)\n",
        "        display(fig)\n",
        "\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        unique, counts = torch.unique(preds, return_counts=True)\n",
        "        print(\"Prediction class distribution:\", dict(zip(unique.tolist(), counts.tolist())))\n"
      ]
    }
  ]
}